```{r}
library(dplyr)

# Load data
training <- read.csv("train.csv")
testing <- read.csv("test.csv")

# Combine the datasets for cleaning
combined <- bind_rows(training, testing, .id = "dataset")

# Check for missing values
print("Missing values before cleaning:")
print(sapply(combined, function(x) sum(is.na(x))))

# Impute missing values with the median
for (col in colnames(combined)) {
  combined[[col]] <- ifelse(is.na(combined[[col]]), median(combined[[col]], na.rm = TRUE), combined[[col]])
}

# Check for missing values after cleaning
print("Missing values after cleaning:")
print(sapply(combined, function(x) sum(is.na(x))))

# Remove duplicate rows
combined <- combined %>% distinct()

# Feature scaling: Normalize all numeric columns
numeric_columns <- sapply(combined, is.numeric)
combined[numeric_columns] <- lapply(combined[numeric_columns], function(x) (x - min(x)) / (max(x) - min(x)))

# Split the cleaned data back into training and testing datasets
cleaned_training <- combined %>% filter(dataset == "1") %>% select(-dataset)
cleaned_testing <- combined %>% filter(dataset == "2") %>% select(-dataset)

# Write the cleaned datasets to CSV files
write.csv(cleaned_training, "cleaned_train.csv", row.names = FALSE)
write.csv(cleaned_testing, "cleaned_test.csv", row.names = FALSE)

```

